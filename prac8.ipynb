{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import stem\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "with open(\"./rt-polaritydata/pos.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        sentences.append(\"+1 \" + line)\n",
    "\n",
    "with open(\"./rt-polaritydata/neg.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        sentences.append(\"-1 \" + line)\n",
    "\n",
    "random.shuffle(sentences)\n",
    "\n",
    "with open(\"./sentiment.txt\", \"w\") as f:\n",
    "    for s in sentences:\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_stopword(word):\n",
    "    return word in count_vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#センテンスを取得\n",
    "def get_sentence():\n",
    "    sentences = []\n",
    "\n",
    "    with open(\"./rt-polaritydata/pos.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            sentences.append({'label':1, 'sentence':line})\n",
    "\n",
    "    with open(\"./rt-polaritydata/neg.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            sentences.append({'label':-1, 'sentence':line})\n",
    "\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature(sentence):\n",
    "    fuature = []\n",
    "    words = \"\"\n",
    "    stemmer = stem.PorterStemmer()\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # 素性の作成\n",
    "    for s in sentence:\n",
    "        for word in re.compile(r'[,.:;\\s]').sub(\" \", s['sentence']).split():\n",
    "            if (not (is_stopword(word))):\n",
    "                words = words + \" \" + stemmer.stem(word)\n",
    "        sentences.append(words)\n",
    "        labels.append(s['label'])\n",
    "        words = \"\"\n",
    "    fuature.append(sentences)\n",
    "    fuature.append(labels)\n",
    "    return fuature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_words_weight(vacabulary, coef):\n",
    "    word_weight = []\n",
    "    for vocab, weight in zip(vacabulary, coef):\n",
    "        word_weight.append({'vocab': vocab, 'weight': weight})\n",
    "\n",
    "    return word_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_predict(X, Y, lr):\n",
    "    true_labels = Y\n",
    "    pre_labels = lr.predict(X)\n",
    "    prob = lr.predict_proba(X)\n",
    "\n",
    "    with open(\"./prob.txt\", \"w\") as f:\n",
    "        for i, t_label in enumerate(true_labels):\n",
    "            if t_label==1:\n",
    "                line = str(t_label) + \"  \" + str(pre_labels[i]) + \"  \"+ str(prob[i][1]) +\"\\n\"\n",
    "            elif t_label==-1:\n",
    "                line = str(t_label) + \"  \" + str(pre_labels[i]) + \"  \"+ str(prob[i][0]) +\"\\n\"\n",
    "\n",
    "            f.writelines(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuature:   [\" love stori 2002' remark process sweep pictur reinvigor romanc genr\", ' movi drag thought heard mysteri voic felt power drawn light -- light exit sign return warn movi 90 minut long life short', ' pretti dicey materi unexpect zig zag help', ' swing fals sentiment unfunni madcap comedi way expect audienc invest central relationship kind marriag true mind', ' jaw-droppingli beauti work upend nearli clich� japanes anim deliv satisfactori carnag', ' battl bug-ey theatr dead-ey matine', ' depress ruthlessli pain deprav movi equival stare open wound', \" don't think [kissinger's] guilti crimin activ contemporari statesmen he'd sure make courtroom trial great fun watch\", ' stupid annoy', \" maneuv skill plot' hot brine -- it' undon soggi contemporari charact actor\", ' thought surprisingli affect portrait screwed-up man dare mess power peopl seen eye idealist kid choos champion ultim lose caus', \" cho' time priceless\", \" choos interpret film' end hope optimist think payn darker\", ' stun dreamlik visual impress viewer littl patienc euro-film pretens', ' play like unbalanc mixtur graphic combat footag saccharin domest interlud pure hollywood', \" shock actual correct interpret shouldn't make movi discuss enjoy\", \" it' enjoy expect that' laugh come fairli basic comed construct cinemat pratfal given work cast spot mood laid\", ' objection dull film mere lack good intent', ' swashbuckl tale love betray reveng faith', \" quit transcend jokest statu punchlin doesn't live barry' dead-ey perfectli chill deliveri\"]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    sentence = get_sentence()\n",
    "    fuature = create_feature(sentence)\n",
    "\n",
    "    print(\"fuature:  \",fuature[0][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary ['barrel', 'barrett', 'barri', 'barrie', 'barrier', 'barris', 'barrow', 'barry', 'barrymor', 'bart', 'bartlebi', 'bartleby', 'bartlett', 'bas', 'base', 'basebal', 'baseball', 'based', 'basest', 'bash', 'basi', 'basic', 'bask', 'basketbal', 'bass', 'bast', 'bastard', 'bastion', 'bat', 'batch', 'bate', 'bath', 'batho', 'bathroom', 'bathtub', 'batman', 'batter', 'batteri', 'battista', 'battl', 'battlefield', 'bawdi', 'baz', 'bazadona', 'beach', 'beachcomb', 'beacon', 'bead', 'beam', 'bean']\n"
     ]
    }
   ],
   "source": [
    "    #単語数のカウント\n",
    "    feature_vectors = count_vectorizer.fit_transform(fuature[0])\n",
    "    vocabulary = count_vectorizer.get_feature_names()\n",
    "\n",
    "    X = feature_vectors.toarray()\n",
    "    Y = fuature[1]\n",
    "\n",
    "    print(\"vocabulary\",vocabulary[1000:1050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"X\",X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "print(Y[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseEstimator.get_params of LogisticRegression(C=1000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)>\n"
     ]
    }
   ],
   "source": [
    "    #学習\n",
    "    lr = LogisticRegression(C=1000.0)\n",
    "    lr.fit(X, Y)\n",
    "\n",
    "    print(lr.get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.55079667e-04   9.99244920e-01]\n"
     ]
    }
   ],
   "source": [
    "    #文章を与えて確率を予測\n",
    "    prob = lr.predict_proba(X[0].reshape(1, -1))[0]\n",
    "    print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top10 [{'vocab': 'badg', 'weight': 20.300929872579335}, {'vocab': 'anatom', 'weight': 19.134890839389183}, {'vocab': 'unsurpass', 'weight': 18.746610509806906}, {'vocab': 'tape', 'weight': 17.731850068991541}, {'vocab': 'taut', 'weight': 17.674107208192794}, {'vocab': 'cloud', 'weight': 17.584325567727856}, {'vocab': 'liber', 'weight': 17.206780720057498}, {'vocab': 'cozi', 'weight': 17.165516349663658}, {'vocab': 'eerili', 'weight': 15.649174340641638}, {'vocab': 'smarter', 'weight': 15.624898836347407}, {'vocab': 'jackie', 'weight': 15.521522162584656}]\n"
     ]
    }
   ],
   "source": [
    "    #単語と重みの紐付け\n",
    "    word_weight = create_words_weight(vocabulary, lr.coef_[0])\n",
    "\n",
    "    #TOP10を抽出する\n",
    "    descend_weight = sorted(word_weight, key=lambda x: x[\"weight\"],reverse=True)\n",
    "    print(\"top10\", descend_weight[0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under10 [{'vocab': '85', 'weight': -19.506948923902453}, {'vocab': 'poorli', 'weight': -18.817113735682703}, {'vocab': 'unless', 'weight': -18.063634799969911}, {'vocab': 'picture', 'weight': -17.55862714103672}, {'vocab': 'rosenth', 'weight': -17.283457652570263}, {'vocab': 'devolv', 'weight': -17.171981581336386}, {'vocab': 'languor', 'weight': -16.752574365306874}, {'vocab': 'jumbl', 'weight': -16.641124615154599}, {'vocab': 'witherspoon', 'weight': -16.609216839911056}, {'vocab': 'scooter', 'weight': -16.387293951530467}, {'vocab': 'relic', 'weight': -16.332322014592869}]\n"
     ]
    }
   ],
   "source": [
    "    #UNDER10を抽出\n",
    "    ascend_weight = sorted(word_weight, key=lambda x: x[\"weight\"])\n",
    "    print(\"under10\", ascend_weight[0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pre = lr.predict(X)\n",
    "    \n",
    "#ラベルと確率をファイル出力\n",
    "export_predict(X, Y, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.999249671731\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       1.00      1.00      1.00      5331\n",
      "          1       1.00      1.00      1.00      5331\n",
      "\n",
      "avg / total       1.00      1.00      1.00     10662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    '''\n",
    "    予測の正解率，正例に関する適合率，再現率，F1スコア\n",
    "    精度(適合率, precision)：正と予測したデータのうち，実際に正であるものの割合\n",
    "    再現率 (recall)：実際に正であるもののうち，正であると予測されたものの割\n",
    "    F値 (F尺度, F-measure)：精度と再現率の調和平均．\n",
    "    '''\n",
    "    #正解率\n",
    "    print('accuracy: ' + str(accuracy_score(Y, y_pre)))\n",
    "    print(classification_report(Y, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    '''\n",
    "    5-分割交差検定\n",
    "    データを5分割して4つを訓練データ、1つをテストデータとして用いる手法\n",
    "    '''\n",
    "    \n",
    "    scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='accuracy')\n",
    "    print(\"accuracy:  \", scores)\n",
    "    scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='precision')\n",
    "    print(\"precision:  \", scores)\n",
    "    scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='recall')\n",
    "    print(\"recall:  \", scores)\n",
    "    scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='f1_weighted')\n",
    "    print(\"f1_weighted:  \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresholds = [{1:0.99, -1:0.01},{1:0.95, -1:0.05},{1:0.9, -1:0.1},{1:0.85, -1:0.15},{1:0.8, -1:0.2}\n",
    "        ,{1:0.75, -1:0.25},{1:0.7, -1:0.3},{1:0.65, -1:0.35},{1:0.6, -1:0.4},{1:0.55, -1:0.45},{1:0.5, -1:0.5}\n",
    "        ,{1:0.45, -1:0.55},{1:0.4, -1:0.6}, {1:0.35, -1:0.7}, {1:0.3, -1:0.7}, {1:0.25, -1:0.8},{1:0.2, -1:0.8}\n",
    "        , {1:0.15, -1:0.85},{1:0.1, -1:0.9}, {1:0.05, -1:0.95}, {1:0.01, -1: 0.99}]\n",
    "    t = [0.99,0.95,0.9,0.85,0.8,0.75,0.7,0.65,0.6,0.55,0.5,0.45,0.4,0.35,0.3,0.25,0.2,0.15,0.1,0.05,0.01]\n",
    "\n",
    "    precision_rates = []\n",
    "    recall_rates = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='precision')\n",
    "        precision_rates.append(scores[0])\n",
    "        scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='recall')\n",
    "        recall_rates.append(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, precision_rates, label=\"precision\", color=\"red\")\n",
    "    plt.plot(thresholds, recall_rates, label=\"recall\", color=\"blue\")\n",
    "\n",
    "    plt.xlabel(\"threshold\")\n",
    "    plt.ylabel(\"rate\")\n",
    "    plt.xlim(-0.05, 0.5)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Logistic Regression\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
