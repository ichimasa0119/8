{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import stem\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "with open(\"./rt-polaritydata/pos.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        sentences.append(\"+1 \" + line)\n",
    "\n",
    "with open(\"./rt-polaritydata/neg.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        sentences.append(\"-1 \" + line)\n",
    "\n",
    "random.shuffle(sentences)\n",
    "\n",
    "with open(\"./sentiment.txt\", \"w\") as f:\n",
    "    for s in sentences:\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop wordとは\n",
    "--------------------------------------\n",
    "<p>自然言語を処理するにあたって一般的であるなどの理由で処理対象外とする単語。</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_stopword(word):\n",
    "    return word in count_vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#センテンスを取得\n",
    "def get_sentence():\n",
    "    sentences = []\n",
    "\n",
    "    with open(\"./rt-polaritydata/pos.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            sentences.append({'label':1, 'sentence':line})\n",
    "\n",
    "    with open(\"./rt-polaritydata/neg.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            sentences.append({'label':-1, 'sentence':line})\n",
    "\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature(sentence):\n",
    "    fuature = []\n",
    "    words = \"\"\n",
    "    stemmer = stem.PorterStemmer()\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # 素性の作成\n",
    "    for s in sentence:\n",
    "        for word in re.compile(r'[,.:;\\s]').sub(\" \", s['sentence']).split():\n",
    "            if (not (is_stopword(word))):\n",
    "                words = words + \" \" + stemmer.stem(word)\n",
    "        sentences.append(words)\n",
    "        labels.append(s['label'])\n",
    "        words = \"\"\n",
    "    fuature.append(sentences)\n",
    "    fuature.append(labels)\n",
    "    return fuature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_words_weight(vacabulary, coef):\n",
    "    word_weight = []\n",
    "    for vocab, weight in zip(vacabulary, coef):\n",
    "        word_weight.append({'vocab': vocab, 'weight': weight})\n",
    "\n",
    "    return word_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_predict(X, Y, lr):\n",
    "    true_labels = Y\n",
    "    pre_labels = lr.predict(X)\n",
    "    prob = lr.predict_proba(X)\n",
    "\n",
    "    with open(\"./prob.txt\", \"w\") as f:\n",
    "        for i, t_label in enumerate(true_labels):\n",
    "            if t_label==1:\n",
    "                line = str(t_label) + \"  \" + str(pre_labels[i]) + \"  \"+ str(prob[i][1]) +\"\\n\"\n",
    "            elif t_label==-1:\n",
    "                line = str(t_label) + \"  \" + str(pre_labels[i]) + \"  \"+ str(prob[i][0]) +\"\\n\"\n",
    "\n",
    "            f.writelines(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuature:   [' terribl movi peopl move', ' superl b movi -- funni sexi rous', ' ultim repel fulli endear american art hous audienc notabl stylist auster forc', ' suspect craven endors simpli movi make look better comparison', ' director mr ratliff wise reject temptat make fun subject', ' birot compet filmmak stori fresh excit', ' harland william funni drag consid perman sex-reassign', ' big build-up payoff audienc charact messi murki unsatisfi', \" just know terribl go happen doe you'r entir unprepar\", \" despit believ goal maker repres spectacular piec theater there' deni talent creativ forc\", \" real movi real peopl give rare glimps cultur don't know\", ' moment insight fondli rememb endlessli challeng maze moviego', ' best film seen constantli pull rug underneath see thing new side plung deeper get intens', \" it' technic superb film shine usual spielberg flair expertli util talent top-notch creativ team\", ' countri bear scene upset frighten young viewer unfortun flat effort amus entertain', \" add sum part holofcener' film offer just insight simplemind ensembl cast engag shift chair\", ' static sugari littl half-hour after-school special interfaith understand stretch 90 minut', ' movi jolt seat coupl time laugh leav feel like worth seven buck doe turn bit cheat end', \" brash intellig erot perplex haneke' portrait upper class austrian societi suppress tuck away demon uniqu felt sardon jolt\", ' shot like postcard overact boozi self-indulg bring worst talent actor']\n",
      "[-1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    sentence = get_sentence()\n",
    "    fuature = create_feature(sentence)\n",
    "\n",
    "    print(\"fuature:  \",fuature[0][0:20])\n",
    "    print(fuature[1][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary ['barrel', 'barrett', 'barri', 'barrie', 'barrier', 'barris', 'barrow', 'barry', 'barrymor', 'bart', 'bartlebi', 'bartleby', 'bartlett', 'bas', 'base', 'basebal', 'baseball', 'based', 'basest', 'bash', 'basi', 'basic', 'bask', 'basketbal', 'bass', 'bast', 'bastard', 'bastion', 'bat', 'batch', 'bate', 'bath', 'batho', 'bathroom', 'bathtub', 'batman', 'batter', 'batteri', 'battista', 'battl', 'battlefield', 'bawdi', 'baz', 'bazadona', 'beach', 'beachcomb', 'beacon', 'bead', 'beam', 'bean']\n"
     ]
    }
   ],
   "source": [
    "    #単語数のカウント\n",
    "    feature_vectors = count_vectorizer.fit_transform(fuature[0])\n",
    "    vocabulary = count_vectorizer.get_feature_names()\n",
    "\n",
    "    X = feature_vectors.toarray()\n",
    "    Y = fuature[1]\n",
    "\n",
    "    print(\"vocabulary\",vocabulary[1000:1050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "print(Y[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseEstimator.get_params of LogisticRegression(C=1000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)>\n"
     ]
    }
   ],
   "source": [
    "    #学習\n",
    "    lr = LogisticRegression(C=1000.0)\n",
    "    lr.fit(X, Y)\n",
    "\n",
    "    print(lr.get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.55079667e-04   9.99244920e-01]\n"
     ]
    }
   ],
   "source": [
    "    #文章を与えて確率を予測\n",
    "    prob = lr.predict_proba(X[0].reshape(1, -1))[0]\n",
    "    print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top10 [{'vocab': 'badg', 'weight': 20.300929872579335}, {'vocab': 'anatom', 'weight': 19.134890839389183}, {'vocab': 'unsurpass', 'weight': 18.746610509806906}, {'vocab': 'tape', 'weight': 17.731850068991541}, {'vocab': 'taut', 'weight': 17.674107208192794}, {'vocab': 'cloud', 'weight': 17.584325567727856}, {'vocab': 'liber', 'weight': 17.206780720057498}, {'vocab': 'cozi', 'weight': 17.165516349663658}, {'vocab': 'eerili', 'weight': 15.649174340641638}, {'vocab': 'smarter', 'weight': 15.624898836347407}, {'vocab': 'jackie', 'weight': 15.521522162584656}]\n"
     ]
    }
   ],
   "source": [
    "    #単語と重みの紐付け\n",
    "    word_weight = create_words_weight(vocabulary, lr.coef_[0])\n",
    "\n",
    "    #TOP10を抽出する\n",
    "    descend_weight = sorted(word_weight, key=lambda x: x[\"weight\"],reverse=True)\n",
    "    print(\"top10\", descend_weight[0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under10 [{'vocab': '85', 'weight': -19.506948923902453}, {'vocab': 'poorli', 'weight': -18.817113735682703}, {'vocab': 'unless', 'weight': -18.063634799969911}, {'vocab': 'picture', 'weight': -17.55862714103672}, {'vocab': 'rosenth', 'weight': -17.283457652570263}, {'vocab': 'devolv', 'weight': -17.171981581336386}, {'vocab': 'languor', 'weight': -16.752574365306874}, {'vocab': 'jumbl', 'weight': -16.641124615154599}, {'vocab': 'witherspoon', 'weight': -16.609216839911056}, {'vocab': 'scooter', 'weight': -16.387293951530467}, {'vocab': 'relic', 'weight': -16.332322014592869}]\n"
     ]
    }
   ],
   "source": [
    "    #UNDER10を抽出\n",
    "    ascend_weight = sorted(word_weight, key=lambda x: x[\"weight\"])\n",
    "    print(\"under10\", ascend_weight[0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pre = lr.predict(X)\n",
    "    \n",
    "#ラベルと確率をファイル出力\n",
    "export_predict(X, Y, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.999249671731\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       1.00      1.00      1.00      5331\n",
      "          1       1.00      1.00      1.00      5331\n",
      "\n",
      "avg / total       1.00      1.00      1.00     10662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    '''\n",
    "    予測の正解率，正例に関する適合率，再現率，F1スコア\n",
    "    精度(適合率, precision)：正と予測したデータのうち，実際に正であるものの割合\n",
    "    再現率 (recall)：実際に正であるもののうち，正であると予測されたものの割\n",
    "    F値 (F尺度, F-measure)：精度と再現率の調和平均．\n",
    "    '''\n",
    "    #正解率\n",
    "    print('accuracy: ' + str(accuracy_score(Y, y_pre)))\n",
    "    print(classification_report(Y, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   [ 0.69447048  0.69371482  0.69559099  0.70356473  0.7021576 ]\n",
      "precision:   [ 0.68576544  0.69245107  0.69395349  0.70471698  0.70349386]\n",
      "recall:   [ 0.71790066  0.69699812  0.69981238  0.70075047  0.6988743 ]\n",
      "f1_weighted:   [ 0.69430266  0.69371152  0.69558557  0.70356238  0.70215439]\n"
     ]
    }
   ],
   "source": [
    "    '''\n",
    "    5-分割交差検定\n",
    "    データを5分割して4つを訓練データ、1つをテストデータとして用いる手法\n",
    "    '''\n",
    "    \n",
    "    scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='accuracy')\n",
    "    print(\"accuracy:  \", scores)\n",
    "    scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='precision')\n",
    "    print(\"precision:  \", scores)\n",
    "    scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='recall')\n",
    "    print(\"recall:  \", scores)\n",
    "    scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='f1_weighted')\n",
    "    print(\"f1_weighted:  \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-67-45f1484f108e>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-67-45f1484f108e>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    precision_rates = []\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "thresholds = [{1:0.99, -1:0.01},{1:0.95, -1:0.05},{1:0.9, -1:0.1},{1:0.85, -1:0.15},{1:0.8, -1:0.2}\n",
    "        ,{1:0.75, -1:0.25},{1:0.7, -1:0.3},{1:0.65, -1:0.35},{1:0.6, -1:0.4},{1:0.55, -1:0.45},{1:0.5, -1:0.5}\n",
    "        ,{1:0.45, -1:0.55},{1:0.4, -1:0.6}, {1:0.35, -1:0.7}, {1:0.3, -1:0.7}, {1:0.25, -1:0.8},{1:0.2, -1:0.8}\n",
    "        , {1:0.15, -1:0.85},{1:0.1, -1:0.9}, {1:0.05, -1:0.95}, {1:0.01, -1: 0.99}]\n",
    "#t = [0.99,0.95,0.9,0.85,0.8,0.75,0.7,0.65,0.6,0.55,0.5,0.45,0.4,0.35,0.3,0.25,0.2,0.15,0.1,0.05,0.01]\n",
    "\n",
    "    precision_rates = []\n",
    "    recall_rates = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='precision')\n",
    "        precision_rates.append(scores[0])\n",
    "        scores = cross_validation.cross_val_score(lr, X, Y, cv=5, scoring='recall')\n",
    "        recall_rates.append(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, precision_rates, label=\"precision\", color=\"red\")\n",
    "    plt.plot(thresholds, recall_rates, label=\"recall\", color=\"blue\")\n",
    "\n",
    "    plt.xlabel(\"threshold\")\n",
    "    plt.ylabel(\"rate\")\n",
    "    plt.xlim(-0.05, 0.5)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Logistic Regression\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
